---
title: "üöô CarScraping"
subtitle: "<script>const currentDT = new Date().toLocaleDateString();document.write(currentDT);</script>"
author: "`Aybuk√© BICAT` & `Hassan TILKI`"
format:
  revealjs:
    slide-number: true
    mermaid-format: svg
    chalkboard: true
    transition: slide
    background-transition: fade
    theme: simple
    incremental: false  
    footer: "--- Machine Learning ---"
    logo: https://corentinducloux.fr/dossier_img/mecen_transparent.png
    controls : true
    preview-links: auto
    view-distance: 20
    fontsize: 2.0em
    reference-location: document
    include-in-header:
        - text: |
            <style>
            #title-slide .title {
            font-size: 1.75em;
            }
            </style>
           
jupyter: python3
---
 
 
 
## Introduction
 
:::{.notes}
on s'est int√©ress√© au march√© de la voiture et on s'est pos√© la question de ce qu'on peut pr√©dire sur les voitures. Quand on pense √† √ßa, on pense forc√©ment au prix, comment pr√©dire le prix d'un v√©hivule?
 
La plupart des fran√ßais utilisent la c√¥te argus mais est-ce que vous vous √™tes d√©j√† demand√© comment celle-ci fonctionne?
 
- Argus :
Elle est bas√©e sur le cours-moyen d'argus qui est constitu√© √† l'aide de professionnels du secteur, d'un concessionnaire allant jusqu'au constructeur, mais aussi √† l'aide de simples particuliers et des annonces de ventes de v√©hicule.
 
 
En apprenant cela, nous avons eu comme objectif de cr√©er notre propre estimateur de prix de voiture.
 
Pour ce faire, vu qu'on est que 2 jeunes √©tudiants et ne sommes pas entour√©s de professionnels, la seule option que nus avons est de scraper des annonces de voitures d'occasion en ligne.
 
Avant de vous montrer comment on a scrapper tout √ßa on va directement passer au + croustillant qui est une d√©monstration de l'application que nous avons cr√©√©e.
 
Pr√©sentation Streamlit:
 
on se retrouvr sur l'accueil de notre app, qui est une app streamlit. On va vous expliquer plus tard ce que c'est.
Sur notre app, nous avons 3 onglets : accueil, acheteur et vendeur.
 
D√©mo acheteur, d√©mo vendeur.
On va pouvoir visualiser
 
d√©mo acheteur : Hassan est-ce que tu es interess√© par l'achat d'un v√©hicule?
 
 
si lien inexistant : c'√©tait tellement une bonne affaire qu'elle est d√©j√† partie...
 
d√©mo vendeur : toutes les donn√©es pr√©sentes dans l'onglet acheteur ont servi √† entrainer les mod√®les de pr√©diction.
 
on a une sorte de transparence sur comment est calcul√©e la c√¥te.
 
Est-ce que vous avez une voiture √† vendre? ou des caract√©ristiques
:::
 
 
 
*CarScraping qu'est-ce que c'est ?*
 
- Comment se calcule la **C√¥te ARGUS** ?
 
> Elle est bas√©e sur le cours-moyen d'argus qui est constitu√© √† l'aide de professionnels du secteur, d'un concessionnaire allant jusqu'au constructeur, mais aussi √† l'aide de simples particuliers et des annonces de ventes de v√©hicule.
 
- Inconv√©nient : Payant, manque de transparence.
 
- **Objectif** : cr√©er notre propre c√¥te ARGUS gratuite et sans filtre  !
 
 
 
## Web Scraping üï∏Ô∏è
 
:::{.notes}
Alors maintenant qu'on a vu le produit final, on va le diss√©quer pour voir comment on en est arriv√© l√† avec une approche technique!
 
Vous avez vu le voir peut √™tre mais il y a des liens dans l'onglet acheteur qui renvoie vers le site "La Centrale".
 
Alors nous avons scrap√© ce site et l'int√©gralit√© des 300 000 annonces qui √©taient pr√©sentes sur le site au moment du scraping.
 
Pour la partie scraping
 
 
On va vous montrer le processus de scraping en 3 parties : r√©cup√©ration des noms des marques, puis la r√©cup√©ration des annonces, et enfin la r√©cup√©ration des caract√©ristiques.
 
pas d'utilisation de Selenium car le site bloque tous les bot.
 
:::
 
> C'est une technique d'extraction de donn√©es pr√©sentes sur des sites  web par l'utilisation d'un script ou programme informatique.
 
Donn√©es r√©cup√©r√©es depuis **La Centrale** :
 
- \+ de **300 000** annonces
- \+ de **150** marques
- \+ de **900** mod√®les
 
 
Extraction en 3 parties :
  - R√©cup√©ration des noms des marques
  - R√©cup√©ration des annonces
  - R√©cup√©ration des caract√©ristiques
 
 
 
## Processus du Scraping : 1/3
 
:::{.notes}
 
On a un module `webscraping.py` qui nous permet d'extraire toutes ces donn√©es depuis le site.
 
 
Il faut savoir que le nombre maximum de pages sur La Centrale est de 500.
 
Avec 16 annonces par page, on n'atteint pas les 300k annonces annonc√©s dans l'accueil du site.
 
Pour y parvenir, nous avons d√©cid√© de scraper 500 pages d'annonces par marque et par mod√®les de v√©hicule √† l'aide de requ√™tes http.
 
Pour obtenir ces diff√©rentes marques, il suffit juste d'aller dans l'accueil du site, de cliquer sur la listbox "Mod√®le" et d'ensuite t√©l√©charger la page.
 
 
 
:::
 
- Utilisation des 2 packages python : `request` et `bs4`.
 
- R√©cup√©ration de la liste des marques et des mod√®les associ√©s.
  
 
- Transformer en format JSON.
 
<div style="max-height: 100px; overflow-y: scroll;">
 
:::{.fragment}
```{json}
{
    "AC": [
        "aceca",
        "cobra"
    ],
    "AIWAYS": [
        "u5"
    ],
}
```
 
:::

## Processus du Scraping : (2/3)
:::{.notes}

Une fois qu'on a r√©cup√©r√© les marques, on peut maintenant √† l'aide d'une boucle et de fonctions bien ficell√©es, extraire toutes les annonces sur le site. 
*extrait f¬∞ recup_pages()*

On va r√©cup√©rer les annonces directement depuis l'url pour pouvoir pallier le probl√®me des 500 pages maximum.

Cette fonction : `extraire_toutes_annonces()`, en contient d'autre. Ce qui permet d'iterer sur chaque marques et modeles disponible, grace au fichier JSON cr√©e pr√©alablement. Ensuite on effectue une requete http a l'aide de requetes (on montre l'exemple de la fonction recup_page).

:::

- Extraction de toutes les annonces disponibles sur le site √† l'aide de la fonction `extract_toutes_annonces()`, elle permet :
  - d'iterer sur chaque marques et mod√®les de v√©hicule
  - d'extraire les pages correspondantes

```py
def recup_page(numero_page: int, marque: str, modele: str) -> BeautifulSoup:
    adresse = f"https://www.lacentrale.fr/listing?makesModelsCommercialNames={marque.upper()}%3A{modele.upper()}&options=&page={numero_page}"
    ...
    requete = rq.get(url=adresse)
    page = BeautifulSoup(requete.content, "html.parser")
``` 
  - d'avoir une sortie sous forme de liste d'annonces
  
```py
def recup_annonces(page: BeautifulSoup) -> list:
    annonces = page.find_all('div', class_='searchCardContainer')
    return list(annonces)
``` 


## Processus du Scraping : (3/3)

:::{.notes}

Une fois toutes les pages r√©cuper√©es, sous cette forme de liste **pointe la slide**, on r√©cup√®re chacune des caract√©ristiques du v"hicule pour chq annonce. 
La fonction permettant de r√©cuperer toutes les caract√©ristiques d'une annonce s'appelle recup_info_voitures() qui elle aussi fait appel √† pls fonctions d√©di√©es √† pls caract√©ristiques. Ces fonctions permette √† l'aide de bs4, de recuperer des information pr√©cise stock√© dans certain √©lement de la page. 

Chacune des caract√©ristiques des voitures est stock√©e dans une dataclass `voiture`. 
:::

![](imgs/sorties_annonces.png){fig-align="center" width="0.5"}

- Extraction des caract√©ristiques des v√©hicules.

```py
def recup_informations_voiture(annonce: str) -> voiture:
    marque = recup_nom_vehicule(annonce)
    cylindre = recup_cylindre(annonce)
    annee, kilometrage, boite, energie = recup_caracteristiques(annonce)
    prix = recup_prix(annonce)
    position_march√© = recup_position_march√©(annonce)
    garantie = recup_garantie(annonce)
    lien = recup_href(annonce)
    return voiture(marque=marque, cylindre=cylindre, annee=annee, kilometrage=kilometrage, boite=boite, energie=energie, prix=prix, position_march√©=position_march√©, garantie=garantie, lien=lien)

def recup_nom_vehicule(annonce: str) -> str: 
    nom_vehicule = annonce.find_all('h2', class_='Text_Text_text Vehiculecard_Vehiculecard_title Text_Text_subtitle2')
    return nom_vehicule[0].text
```

----


:::{.notes}
Et une fois le tout execut√©, les donn√©es sont export√©s sous format brut JSON. 

le nb d'annonces √©tant tr√®s cons√©quent et l'extraction √©tant tr√®s longue, nous avons fait l'extraction en 4 parties. Nous avons obtenus donc  'fichiers json qu'on a fusionner en un fichier √† l'aide de la f¬∞ `fusionner_fichiers_json()`. 
:::

‚úÖ R√©sultats 4 fichiers JSON obtenu :


```
json/
  {} data_a_d.json
  {} data_e_l.json
  {} data_m_p.json
  {} data_r_z.json
```

Un extrait du fichier JSON obtenu : 
```{json}
[
    {
        "marque": "CITROEN C3 III",
        "cylindre": "1.2 PURETECH 110 FEEL",
        "annee": "2019",
        "kilometrage": "10 698 km",
        "boite": "Automatique",
        "energie": "Essence",
        "prix": "15 990 ‚Ç¨",
        "position_march√©": "Bonne affaire",
        "garantie": "Garantie 12 mois",
        "lien": "https://www.lacentrale.fr/auto-occasion-annonce-69112858137.html"
    },
    ...
]

```

On les fusionne √† l'aide de la fonction `fusionner_fichiers_json` pour obtenir un seul fichier contenant toutes les donn√©es extraites.



## üßº Data cleaning üßπ

:::{.notes}

Une fois notre fichier json obtenue, chose inevitable nous avons des donn√©es mal renseign√©es, des pb de types de donn√©es mais aussi des informations √† extraire de ces donn√©es brutes.

Pour r√©cup√©rer le + d'informations possibles sur ces donn√©es, nous avons d√©cid√© d'utiliser la librairie polars, ce qui rend le tout tr√®s lisible et tr√®s efficace.

:::

- Essentiel pour pouvoir exploiter les donn√©es.
- Permet de r√©cup√©rer + d'informations.


Pour le nettoyage de la base de donn√©es, nous avons utilis√© le package `Polars` üêª‚Äç‚ùÑÔ∏è, qui poss√®de de nombreux avantages comme :  

-   la cr√©ation de pipelines üõ¢Ô∏è
-   un code plus lisible üëÄ
-   dans certains cas, + efficace que `Pandas` üêº

C'est le module `datacleaning.py` qui contient toutes les fonctions essentiels pour r√©soudre certains probl√®mes.

## Les probl√®mes rencontr√©es ‚ö†Ô∏è

:::{.notes}

Dans cette partie, nous allons vous pr√©senter rapidement les pb qu'on a rncontr√©es, et les fonctions qui ont servi √† les r√®gler et qui nous ont permis de nettoyer la base de donn√©es et d'avoir une BDD exploitable.


- modele_marque_generation

pb rencontr√©, nom de la f¬∞, et la sortie.  (faire figurer l'ex des docstring)

:::

- S√©parer la marque, le mod√®le et la g√©n√©ration √† l'aide de `get_marque_modele_generation()`

> [`'CITROEN C3 III'`]{.fragment .semi-fade-out} [->]{.fragment} [`('CITROEN', 'C3', 'III')`]{.fragment}

- Corriger le type des variables `kilometre`, `prix` et `annee` avec `get_km_prix_annee` : 

> [`"100 000 km", "15 000 ‚Ç¨", "2015"`]{.fragment .semi-fade-out} [->]{.fragment} [`(100000, 15000, 2015)`]{.fragment}

- Diviser la chaine de caractere de `cylindre` en plusieurs variable avec `get_cylindre`: 

> [`"6.3 V8 460 GT"`]{.fragment .semi-fade-out} [->]{.fragment} [`('6.3', 'V8', 460, 'GT')`]{.fragment}

## Le gazoduc üõ¢Ô∏è

:::{.notes}
f¬∞ qui permet de faire tout le traitement automatiquement √† l'aide des autres fonctions. 

Comme on peut le voir, il y a d'autre fonction dans cette pipeline que l'on ne vous a pas pr√©sent√©. On va donc le faire tr√®s rapidement. 

get_garantie traite les donn√©es de la colonne "garantie" dans en transformant le Garantie 12 mois en gardant uniquement les valeurs num√©riques. Elle contient aussi d'autre filtre pour √©carter les valeurs ab√©rantes 

filter_data permet de supprimer les valeurs ab√©rante pour le kilom√©trage et l'ann√©e 2024. 

supp_doublons permet de supprimer les annonces en double en ce basant sur les liens des annonces. 

supp_na: Elle permet aussi de supprimer les valeurs manquantes pour la puissance. et pour les marques et modeles de v√©hicule

Une fois le dataframe nettoy√© il sera export√© en format parquet car parquet a une Compression efficace et permet d'avoir des √©conomies de stockage. 
:::


C'est une fonction qui applique un pipeline de traitement de donn√©es sur le DataFrame donn√©.
 
```python
def gazoduc(data: pl.DataFrame, nom_marques_modeles: pl.DataFrame) -> pl.DataFrame:
    data = (data.pipe(get_marque_modele_generation, nom_marques_modeles)üõ¢Ô∏è
                .pipe(get_km_prix_annee)                                üõ¢Ô∏è
                .pipe(get_garantie)                                     üõ¢Ô∏è
                .pipe(get_cylindre)                                     üõ¢Ô∏è
                .pipe(filter_data)                                      üõ¢Ô∏è
                .pipe(supp_doublons)                                    üõ¢Ô∏è  
                .pipe(supp_na)                                          üõ¢Ô∏è
        )
    return data
```

- Extrait dataframe

Ce dataframe sera export√© en format parquet qui poss√®de un avantage important : 
-   permet de stocker des bases volumineuse. 

## Machine learning


:::{.notes}
- split
- les mod√®les -> pourquoi avoir choisi ces mod√®les ? car apr√®s test, ces mod√®les se sont aver√©e √™tre meilleurs.
- les grilles de param√®tres associ√©es -> pourquoi ces grilles ? car apr√®s test, se sont les r√©sultats qui sont sortie le plus souvent.
- trouver le meilleur mod√®le (CV) -> on montre la fonction 
- export du Mod√®le et pr√©processeur
- import du mod√®le et utilisation pour pr√©diction

:::




## Streamlit


:::{.notes}
expliquer streamlit
- onglet acheteur : fonctionnement avec des requ√™tes, 
- onglet vendeur : import du mod√®le et pr√©dictions estimation du prix 



:::
