---
title: "ğŸš™ CarScraping ğŸš™"
subtitle: "<script>const currentDT = new Date().toLocaleDateString();document.write(currentDT);</script>"
author: "`AybukÃ© BICAT` & `Hassan TILKI`"
format:
  revealjs:
    slide-number: true
    mermaid-format: svg
    chalkboard: true
    transition: slide
    background-transition: fade
    theme: simple
    incremental: true  
    footer: "--- Machine Learning ---"
    logo: https://corentinducloux.fr/dossier_img/mecen_transparent.png
    controls : true
    preview-links: auto
    view-distance: 20
    fontsize: 2.0em
    reference-location: document
    include-in-header:
        - text: |
            <style>
            #title-slide .title {
            font-size: 1.75em;
            }
            </style>
           
jupyter: python3
---



## Introduction

:::{.notes}
on s'est intÃ©ressÃ© au marchÃ© de la voiture et on s'est posÃ© la question de ce qu'on peut prÃ©dire sur les voitures. Quand on pense Ã  Ã§a, on pense forcÃ©ment au prix, comment prÃ©dire le prix d'un vÃ©hivule?

La plupart des franÃ§ais utilisent la cÃ´te argus mais est-ce que vous vous Ãªtes dÃ©jÃ  demandÃ© comment celle-ci fonctionne?

- Argus :
Elle est basÃ©e sur le cours-moyen d'argus qui est constituÃ© Ã  l'aide de professionnels du secteur, d'un concessionnaire allant jusqu'au constructeur, mais aussi Ã  l'aide de simples particuliers et des annonces de ventes de vÃ©hicule.


En apprenant cela, nous avons eu comme objectif de crÃ©er notre propre estimateur de prix de voiture.

Pour ce faire, vu qu'on est que 2 jeunes Ã©tudiants et ne sommes pas entourÃ©s de professionnels, la seule option que nus avons est de scraper des annonces de voitures d'occasion en ligne.

Avant de vous montrer comment on a scrapper tout Ã§a on va directement passer au + croustillant qui est une dÃ©monstration de l'application que nous avons crÃ©Ã©e.

PrÃ©sentation Streamlit:

on se retrouvr sur l'accueil de notre app, qui est une app streamlit. On va vous expliquer plus tard ce que c'est.
Sur notre app, nous avons 3 onglets : accueil, acheteur et vendeur.

DÃ©mo acheteur, dÃ©mo vendeur.
On va pouvoir visualiser

dÃ©mo acheteur : Hassan est-ce que tu es interessÃ© par l'achat d'un vÃ©hicule?


si lien inexistant : c'Ã©tait tellement une bonne affaire qu'elle est dÃ©jÃ  partie...

dÃ©mo vendeur : toutes les donnÃ©es prÃ©sentes dans l'onglet acheteur ont servi Ã  entrainer les modÃ¨les de prÃ©diction.

on a une sorte de transparence sur comment est calculÃ©e la cÃ´te.

Est-ce que vous avez une voiture Ã  vendre? ou des caractÃ©ristiques
:::



*CarScraping qu'est-ce que c'est ?*

- Comment se calcule la **CÃ´te ARGUS** ?

:::{.fragment}
> Elle est basÃ©e sur le cours-moyen d'argus qui est constituÃ© Ã  l'aide de professionnels du secteur, d'un concessionnaire allant jusqu'au constructeur, mais aussi Ã  l'aide de simples particuliers et des annonces de ventes de vÃ©hicule.
:::

- **InconvÃ©nients** : Payant, manque de transparence.

- **Objectif** : crÃ©er notre propre cÃ´te ARGUS gratuite et sans filtre  !


## Web Scraping ğŸ•¸ï¸ {background-image="imgs/webscraping.jpeg" background-opacity="0.1"}

:::{.notes}
Alors maintenant qu'on a vu le produit final, on va le dissÃ©quer pour voir comment on en est arrivÃ© lÃ  avec une approche technique!

Vous avez vu le voir peut Ãªtre mais il y a des liens dans l'onglet acheteur qui renvoie vers le site "La Centrale".

Alors nous avons scrapÃ© ce site et l'intÃ©gralitÃ© des 300 000 annonces qui Ã©taient prÃ©sentes sur le site au moment du scraping.

Pour la partie scraping


On va vous montrer le processus de scraping en 3 parties : rÃ©cupÃ©ration des noms des marques, puis la rÃ©cupÃ©ration des annonces, et enfin la rÃ©cupÃ©ration des caractÃ©ristiques.

pas d'utilisation de Selenium car le site bloque tous les bot.

:::

> C'est une technique d'extraction de donnÃ©es prÃ©sentes sur des sites  web par l'utilisation d'un script ou programme informatique.

:::{.fragment}
DonnÃ©es rÃ©cupÃ©rÃ©es depuis **La Centrale** :
:::

- \+ de <span style="color:#4682B4">**300 000**</span> annonces
- \+ de <span style="color:#4682B4">**150**</span> marques
- \+ de <span style="color:#4682B4">**900**</span> modÃ¨les

:::{.fragment}
Extraction en **3 parties** :

  - RÃ©cupÃ©ration des noms des marques
  - RÃ©cupÃ©ration des annonces
  - RÃ©cupÃ©ration des caractÃ©ristiques
:::


## Processus du Scraping : 1/3

:::{.notes}

On a un module `webscraping.py` qui nous permet d'extraire toutes ces donnÃ©es depuis le site.


Il faut savoir que le nombre maximum de pages sur La Centrale est de 500.

Avec 16 annonces par page, on n'atteint pas les 300k annonces annoncÃ©s dans l'accueil du site.

Pour y parvenir, nous avons dÃ©cidÃ© de scraper 500 pages d'annonces par marque et par modÃ¨les de vÃ©hicule Ã  l'aide de requÃªtes http.

Pour obtenir ces diffÃ©rentes marques, il suffit juste d'aller dans l'accueil du site, de cliquer sur la listbox "ModÃ¨le" et d'ensuite tÃ©lÃ©charger la page.



:::

- Utilisation des 2 packages python : `request` et `bs4`.

- RÃ©cupÃ©ration de la liste des marques et des modÃ¨les associÃ©s.

:::{.fragment}
![](imgs/modelelistbox.png){fig-align="center" height="240"}
:::

- Transformer en format JSON.



:::{.fragment}
```{json}
{   "AC": [
        "aceca",
        "cobra"
    ],
    "AIWAYS": [
        "u5"
    ], ...}
```

:::

## Processus du Scraping : (2/3)
:::{.notes}

Une fois qu'on a rÃ©cupÃ©rÃ© les marques, on peut maintenant Ã  l'aide d'une boucle et de fonctions bien ficellÃ©es, extraire toutes les annonces sur le site. 
*extrait fÂ° recup_pages()*

On va rÃ©cupÃ©rer les annonces directement depuis l'url pour pouvoir pallier le problÃ¨me des 500 pages maximum.

Cette fonction : `extraire_toutes_annonces()`, en contient d'autre. Ce qui permet d'iterer sur chaque marques et modeles disponible, grace au fichier JSON crÃ©e prÃ©alablement. Ensuite on effectue une requete http a l'aide de requetes (on montre l'exemple de la fonction recup_page).

:::

- Extraction de toutes les annonces disponibles sur le site Ã  l'aide de la fonction `extract_toutes_annonces()`, elle permet :
  - d'iterer sur chaque marques et modÃ¨les de vÃ©hicule
  - d'extraire les pages correspondantes
  
:::{.fragment}
```{py}
def recup_page(numero_page: int, marque: str, modele: str) -> BeautifulSoup:
    adresse = f"https://www.lacentrale.fr/listing?makesModelsCommercialNames={marque.upper()}%3A{modele.upper()}&options=&page={numero_page}"
    ...
    requete = rq.get(url=adresse)
    page = BeautifulSoup(requete.content, "html.parser")
``` 
  - d'avoir une sortie sous forme de liste d'annonces
:::

:::{.fragment}
```{py}
def recup_annonces(page: BeautifulSoup) -> list:
    annonces = page.find_all('div', class_='searchCardContainer')
    return list(annonces)
``` 

:::

## Processus du Scraping : (3/3)

:::{.notes}

Une fois toutes les pages rÃ©cuperÃ©es, sous cette forme de liste **pointe la slide**, on rÃ©cupÃ¨re chacune des caractÃ©ristiques du v"hicule pour chq annonce. 
La fonction permettant de rÃ©cuperer toutes les caractÃ©ristiques d'une annonce s'appelle recup_info_voitures() qui elle aussi fait appel Ã  pls fonctions dÃ©diÃ©es Ã  pls caractÃ©ristiques. Ces fonctions permette Ã  l'aide de bs4, de recuperer des information prÃ©cise stockÃ© dans certain Ã©lement de la page. 

Chacune des caractÃ©ristiques des voitures est stockÃ©e dans une dataclass `voiture`. 
:::

![](imgs/sorties_annonces.png){fig-align="center" width="0.5"}

- Extraction des caractÃ©ristiques des vÃ©hicules.

:::{.fragment}

```{py}
def recup_informations_voiture(annonce: str) -> voiture:
    marque = recup_nom_vehicule(annonce)
    cylindre = recup_cylindre(annonce)
    annee, kilometrage, boite, energie = recup_caracteristiques(annonce)
    prix = recup_prix(annonce)
    position_marchÃ© = recup_position_marchÃ©(annonce)
    garantie = recup_garantie(annonce)
    lien = recup_href(annonce)
    return voiture(marque=marque, cylindre=cylindre, annee=annee, kilometrage=kilometrage, boite=boite, energie=energie, prix=prix, position_marchÃ©=position_marchÃ©, garantie=garantie, lien=lien)

def recup_nom_vehicule(annonce: str) -> str: 
    nom_vehicule = annonce.find_all('h2', class_='Text_Text_text Vehiculecard_Vehiculecard_title Text_Text_subtitle2')
    return nom_vehicule[0].text
```

:::
----


:::{.notes}
Et une fois le tout executÃ©, les donnÃ©es sont exportÃ©s sous format brut JSON. 

le nb d'annonces Ã©tant trÃ¨s consÃ©quent et l'extraction Ã©tant trÃ¨s longue, nous avons fait l'extraction en 4 parties. Nous avons obtenus donc  'fichiers json qu'on a fusionner en un fichier Ã  l'aide de la fÂ° `fusionner_fichiers_json()`. 
:::

âœ… RÃ©sultats 4 fichiers JSON obtenu :


```
json/
  {} data_a_d.json
  {} data_e_l.json
  {} data_m_p.json
  {} data_r_z.json
```


Un extrait du fichier JSON obtenu : 
```{json}
[
    {
        "marque": "CITROEN C3 III",
        "cylindre": "1.2 PURETECH 110 FEEL",
        "annee": "2019",
        "kilometrage": "10 698 km",
        "boite": "Automatique",
        "energie": "Essence",
        "prix": "15 990 â‚¬",
        "position_marchÃ©": "Bonne affaire",
        "garantie": "Garantie 12 mois",
        "lien": "https://www.lacentrale.fr/auto-occasion-annonce-69112858137.html"
    },
    ...
]

```

On les fusionne Ã  l'aide de la fonction `fusionner_fichiers_json` pour obtenir un seul fichier contenant toutes les donnÃ©es extraites.



## ğŸ§¼ Data cleaning ğŸ§¹{background-image="imgs/datacleaning3.jpeg" background-opacity="0.2"}


:::{.notes}

Une fois notre fichier json obtenue, chose inevitable nous avons des donnÃ©es mal renseignÃ©es, des pb de types de donnÃ©es mais aussi des informations Ã  extraire de ces donnÃ©es brutes.

Pour rÃ©cupÃ©rer le + d'informations possibles sur ces donnÃ©es, nous avons dÃ©cidÃ© d'utiliser la librairie polars, ce qui rend le tout trÃ¨s lisible et trÃ¨s efficace.

:::

- Essentiel pour pouvoir exploiter les donnÃ©es.
- Permet de rÃ©cupÃ©rer + d'informations.

:::{.fragment}
Pour le nettoyage de la base de donnÃ©es, nous avons utilisÃ© le package `Polars` ğŸ»â€â„ï¸, qui possÃ¨de de nombreux avantages comme :  
:::
-   la crÃ©ation de pipelines ğŸ›¢ï¸
-   un code plus lisible ğŸ‘€
-   dans certains cas, + efficace que `Pandas` ğŸ¼

:::{.fragment}
C'est le module `datacleaning.py` qui contient toutes les fonctions essentiels pour rÃ©soudre certains problÃ¨mes.
:::

## Les problÃ¨mes rencontrÃ©es âš ï¸

:::{.notes}

Dans cette partie, nous allons vous prÃ©senter rapidement les pb qu'on a rncontrÃ©es, et les fonctions qui ont servi Ã  les rÃ¨gler et qui nous ont permis de nettoyer la base de donnÃ©es et d'avoir une BDD exploitable.


- modele_marque_generation

pb rencontrÃ©, nom de la fÂ°, et la sortie.  (faire figurer l'ex des docstring)

:::

- SÃ©parer la marque, le modÃ¨le et la gÃ©nÃ©ration Ã  l'aide de `get_marque_modele_generation()`

:::{.fragment}
$\rightarrow$ [`'CITROEN C3 III'`]{.fragment .semi-fade-out} [->]{.fragment} [`('CITROEN', 'C3', 'III')`]{.fragment}
:::

- Corriger le type des variables `kilometre`, `prix` et `annee` avec `get_km_prix_annee` : 

:::{.fragment}
$\rightarrow$ [`"100 000 km", "15 000 â‚¬", "2015"`]{.fragment .semi-fade-out} [->]{.fragment} [`(100000, 15000, 2015)`]{.fragment}
:::

- Diviser la chaine de caractere de `cylindre` en plusieurs variables avec `get_cylindre`: 

:::{.fragment}
$\rightarrow$ [`"6.3 V8 460 GT"`]{.fragment .semi-fade-out} [->]{.fragment} [`('6.3', 'V8', 460, 'GT')`]{.fragment}
:::


## Le gazoduc ğŸ›¢ï¸{background-image="imgs/gazoduc1.jpeg" background-opacity="0.1"}

:::{.notes}
fÂ° qui permet de faire tout le traitement automatiquement Ã  l'aide des autres fonctions. 

Comme on peut le voir, il y a d'autre fonction dans cette pipeline que l'on ne vous a pas prÃ©sentÃ©. On va donc le faire trÃ¨s rapidement. 

get_garantie traite les donnÃ©es de la colonne "garantie" dans en transformant le Garantie 12 mois en gardant uniquement les valeurs numÃ©riques. Elle contient aussi d'autre filtre pour Ã©carter les valeurs abÃ©rantes 

filter_data permet de supprimer les valeurs abÃ©rante pour le kilomÃ©trage et l'annÃ©e 2024. 

supp_doublons permet de supprimer les annonces en double en ce basant sur les liens des annonces. 

supp_na: Elle permet aussi de supprimer les valeurs manquantes pour la puissance. et pour les marques et modeles de vÃ©hicule

Une fois le dataframe nettoyÃ© il sera exportÃ© en format parquet car parquet a une Compression efficace et permet d'avoir des Ã©conomies de stockage. 
:::


C'est une fonction qui applique un pipeline de traitement de donnÃ©es sur le DataFrame donnÃ©.

:::{.fragment}
```py
def gazoduc(data: pl.DataFrame, nom_marques_modeles: pl.DataFrame) -> pl.DataFrame:
    data = (data.pipe(get_marque_modele_generation, nom_marques_modeles)ğŸ›¢ï¸
                .pipe(get_km_prix_annee)                                ğŸ›¢ï¸
                .pipe(get_garantie)                                     ğŸ›¢ï¸
                .pipe(get_cylindre)                                     ğŸ›¢ï¸
                .pipe(filter_data)                                      ğŸ›¢ï¸
                .pipe(supp_doublons)                                    ğŸ›¢ï¸  
                .pipe(supp_na)                                          ğŸ›¢ï¸
        )
    return data
```
:::

:::{.fragment}
<div style="overflow-x: auto;">
  <table style="max-width: 800px; font-size: 18px;">
    <caption>Extrait du dataframe</caption>
    <thead>
      <tr>
        <th>annÃ©e</th>
        <th>kilomÃ©trage</th>
        <th>boÃ®te</th>
        <th>Ã©nergie</th>
        <th>prix</th>
        <th>position marchÃ©</th>
        <th>garantie</th>
        <th>lien</th>
        <th>marque</th>
        <th>modÃ¨le</th>
        <th>gÃ©nÃ©ration</th>
        <th>cylindre</th>
        <th>moteur</th>
        <th>puissance</th>
        <th>finition</th>
        <th>batterie</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>2020</td>
        <td>107410</td>
        <td>Manuelle</td>
        <td>Essence</td>
        <td>13050</td>
        <td>Offre Ã©quitable</td>
        <td>12</td>
        <td><a href="https://www.lacentrale.fr/auto-occasion-annonc...">Lien vers l'annonce</a></td>
        <td>CITROEN</td>
        <td>C3</td>
        <td>III</td>
        <td>1.2</td>
        <td>PURETECH</td>
        <td>82</td>
        <td>S&S FEEL BUSINESS</td>
        <td>None</td>
        <td>-</td>
      </tr>
    </tbody>
  </table>
</div>
:::

:::{.fragment}
Ce dataframe sera exportÃ© en format `.parquet` qui permet de stocker des bases volumineuse Ã  moindre coÃ»t. 
:::

## ğŸ“– Machine learning ğŸ“–{background-image="imgs/ML1.jpeg" background-opacity="0.1"}



:::{.notes}
- split
- les modÃ¨les -> pourquoi avoir choisi ces modÃ¨les ? car aprÃ¨s test, ces modÃ¨les se sont averÃ©e Ãªtre meilleurs.
- les grilles de paramÃ¨tres associÃ©es -> pourquoi ces grilles ? car aprÃ¨s test, se sont les rÃ©sultats qui sont sortie le plus souvent.
- trouver le meilleur modÃ¨le (CV) -> on montre la fonction 
- export du ModÃ¨le et prÃ©processeur
- import du modÃ¨le et utilisation pour prÃ©diction

:::

Pour des raisons d'efficacitÃ© pour la prÃ©diction, **chaque marque de vÃ©hicule aura son propre modÃ¨le de prÃ©diction**.

Ces modÃ¨les de prÃ©diction ne peuvent Ãªtre choisi au hasard. Il faut effectuer des tests et choisir le modÃ¨le le plus adaptÃ©.



- package utilisÃ© : `sklearn`

:::{.fragment}
L'utilisation de ce package Ã  plusieurs avantages, tel que sa documentation et une utilisation avec des **pipelines** possibles. Cependant, il ne fonctionne pas avec des bases de donnÃ©es `Polars`.
:::

- split des donnÃ©es avec `split_data`
- preprocesseur avec `get_preprocessor`
- modÃ¨les et grille de paramÃ¨tres avec `set_models` et `get_params`
- pour rÃ©cupÃ©rer et exporter les meilleurs modÃ¨les : `get_all_models`
- `predict_prix` pour charger le modÃ¨le et prÃ©dire

## Le split ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦

:::{.fragment}
**DonnÃ©es d'entrainement** : Ã©chantillon de <span style="color:#4682B4">**80%**</span> d'observations de la marque

**DonnÃ©es de test** : les <span style="color:#4682B4">**20%**</span> restants.
:::

:::{.fragment}
Le split est effectuÃ© Ã  l'aide de `split_data` : 

```py
def split_data(data: pl.DataFrame, marque: str) -> tuple[pl.DataFrame, pl.DataFrame, pd.DataFrame, pd.DataFrame, np.ndarray, np.ndarray]:

    X = data.filter(pl.col("marque") == marque)
    y = X.select('prix')
    X = X.select(pl.exclude("position_marchÃ©")
                ).select(pl.exclude("lien")
                        ).select(pl.exclude("garantie")
                                ).select(pl.exclude("prix"))

    # Split des donnÃ©es en ensembles d'entraÃ®nement et de test
    X_train, X_test, y_train, y_test = train_test_split(X.to_pandas(), y.to_numpy(), test_size=0.2, random_state=21)
    return X, y, X_train, X_test, y_train, y_test

```
:::

## Le preprocesseur â¹ï¸

La plupart des modÃ¨les ne supportent pas les variables qualitatives et requiert donc un prÃ©traitement. 

:::{.fragment}
`OneHotEncoder` est utilisÃ© pour convertir les caractÃ©ristiques catÃ©gorielles en une reprÃ©sentation binaire.

Exemple : 

$\rightarrow$ [`'boite' = 'Automatique''`]{.fragment .semi-fade-out} [->]{.fragment} [`boite.Automatique = 1`]{.fragment}
:::

:::{.fragment}
`StandardScaler` est utilisÃ© pour effectuer une mise Ã  l'Ã©chelle des caractÃ©ristiques numÃ©riques. Il standardise les donnÃ©es en soustrayant la moyenne et en divisant par l'Ã©cart type, de sorte que chaque caractÃ©ristique ait une moyenne de zÃ©ro et une variance de un.
:::

## Les modÃ¨les ğŸ“Š

:::{.notes}
Nous avons testÃ© diffÃ©rents modÃ¨les tel que les SVM, les Boosting, etc.. Mais les meilleurs modÃ¨les Ã  chaque fois Ã©taient : (ceux sur la slide)

Le nombre de modÃ¨le Ã  creer Ã©tant de 40, le procÃ©dÃ©s est assez long. C'esst pour cela que pour optimiser notre temps nous avons dÃ©cider de tester certaines grille de parametres en amont pour en determiner une finale, la plus rÃ©duite possible.

Nous allons donc rapidement vous prÃ©sentez les rÃ©sultats prÃ©liminaires qui nous on permis d'avoir une grille finale.

:::

AprÃ¨s quelques tests, nous avons dÃ©cidÃ© d'utiliser uniquement les modÃ¨les suivant : 

- Regression linÃ©aire ğŸ“ˆ
- K-neighbors ğŸ‘¬ğŸ‘­
- Random Forest ğŸŒ³

:::{.fragment}
La quantitÃ© de donnÃ©es Ã©tant importantes, l'optimisation des paramÃ¨tres peut-Ãªtre trÃ¨s longues.â³
::: 

:::{.fragment}
C'est pour cette raison que plusieurs grilles ont Ã©tÃ© testÃ© en amont afin de determiner une grille finale, voici les rÃ©sultats intermÃ©diaires.
:::

## Le nombre de voisins ğŸ‘¬ğŸ‘¬ğŸ‘¬

:::{.notes}
Au lieu de se lancer directement dans des validations croisÃ©es avec un grille de paramÃ¨tres choisi alÃ©atoirement, 


Nous avons testÃ© des grilles de paramÃ¨tres entre 0 et 15 voisins. Le maximum retenu Ã©tait 10 voisins. Cette pour cette raison que nous avons retenu cette grille.
:::

:::{.fragment}
```{python}
import numpy as np
import polars as pl
import duckdb
import plotly.express as px
marques_array = duckdb.sql(
    """Select marque from (
    SELECT COUNT(*) as nb_annonces,
    marque
    FROM 'data/database.parquet'
    GROUP BY marque
    ORDER BY nb_annonces DESC)
    """).pl().head(40)
marques_array = marques_array.to_numpy()
def cv_result_into_df(modele: str, marques_array: np.ndarray): 
    for marque in marques_array:
        if marque[0] == marques_array[0,0]:
            cv_results = duckdb.sql(
                f"""
                SELECT '{marque[0]}' as marque,
                mean_test_score,
                '{modele}' as modele,
                params
                FROM 'cv_results_neighbors/{marque[0]}_{modele}_results.json'
                WHERE rank_test_score = 1
                """).pl().unnest('params')
        else :
            cv_results_2 = duckdb.sql(
                f"""
                SELECT '{marque[0]}' as marque,
                mean_test_score,
                '{modele}' as modele,
                params
                FROM 'cv_results_neighbors/{marque[0]}_{modele}_results.json'
               
                WHERE rank_test_score = 1
                """).pl().unnest('params')
            cv_results = pl.concat([cv_results, cv_results_2], rechunk=True)
    return cv_results

px.bar(cv_result_into_df("KNeighbors", marques_array), x='marque', y='mean_test_score', color = 'n_neighbors', color_continuous_scale=px.colors.sequential.Blues)
```
:::

:::{.fragment}
La grille retenue: `'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]`
:::

## min_samples_leaf ğŸƒ

:::{.fragment}
```{python}

def cv_result_into_df(modele: str, marques_array: np.ndarray): 
    for marque in marques_array:
        if marque[0] == marques_array[0,0]:
            cv_results = duckdb.sql(
                f"""
                SELECT '{marque[0]}' as marque,
                mean_test_score,
                '{modele}' as modele,
                params
                FROM 'cv_results_leaf/{marque[0]}_{modele}_results.json'
                WHERE rank_test_score = 1
                """).pl().unnest('params')
        else :
            cv_results_2 = duckdb.sql(
                f"""
                SELECT '{marque[0]}' as marque,
                mean_test_score,
                '{modele}' as modele,
                params
                FROM 'cv_results_leaf/{marque[0]}_{modele}_results.json'
               
                WHERE rank_test_score = 1
                """).pl().unnest('params')
            cv_results = pl.concat([cv_results, cv_results_2], rechunk=True)
    return cv_results

px.bar(cv_result_into_df("RandomForest", marques_array), x='marque', y='mean_test_score', color = 'min_samples_leaf', color_continuous_scale=px.colors.sequential.Mint)
```
:::

:::{.fragment}
La grille retenue: `'min_samples_leaf': [2]`
:::

## n_estimators
:::{.notes}
Ici on avait testÃ© plusieurs valeurs allant de 100 Ã  1000. 

On a dÃ©cidÃ© de garder uniquement 500 et 800

:::

:::{.fragment}
```{python}

def cv_result_into_df(modele: str, marques_array: np.ndarray): 
    for marque in marques_array:
        if marque[0] == marques_array[0,0]:
            cv_results = duckdb.sql(
                f"""
                SELECT '{marque[0]}' as marque,
                mean_test_score,
                '{modele}' as modele,
                params
                FROM 'cv_results_nestimators_2/{marque[0]}_{modele}_results.json'
                WHERE rank_test_score = 1
                """).pl().unnest('params')
        else :
            cv_results_2 = duckdb.sql(
                f"""
                SELECT '{marque[0]}' as marque,
                mean_test_score,
                '{modele}' as modele,
                params
                FROM 'cv_results_nestimators_2/{marque[0]}_{modele}_results.json'
               
                WHERE rank_test_score = 1
                """).pl().unnest('params')
            cv_results = pl.concat([cv_results, cv_results_2], rechunk=True)
    return cv_results

px.bar(cv_result_into_df("RandomForest", marques_array), x='marque', y='mean_test_score', color = 'n_estimators', color_continuous_scale=px.colors.sequential.Blues)
```
:::

:::{.fragment}
La grille retenue: `'n_estimators': [500, 800]`
:::

## L'utilisation des modÃ¨les

:::{.notes}
Une fois l'optimisation des paramÃ¨tres effectuÃ© via une seconde validation croisÃ©e. On garde le meilleur modÃ¨le parmis les trois, on l'entraine avec tout le dataset pour la marque en question, et on l'export en fichier joblib, ainsi que le preprocesseur pour rÃ©utilisation. 
:::

Une fois que les meilleurs modÃ¨les ont Ã©tÃ© trouvÃ©s on les exporte :

:::{.fragment}
```py
def export_models(model: LinearRegression|KNeighborsRegressor|RandomForestRegressor, preprocessor: ColumnTransformer, marque: str) -> None:
  # Sauvegarder le modÃ¨le
  dump(model, f'models/{marque}_best_model.joblib')
  # Sauvegarder le prÃ©processeur
  dump(preprocessor, f'models/{marque}_preprocessor.joblib')
```
:::

:::{.fragment}
Pour ensuite les importer lorsque l'on voudra prÃ©dire les prix ! ğŸ’°
:::

:::{.fragment}
```py
def predict_prix(data: pl.DataFrame, marque: str) -> np.ndarray:
    try:
        modele, preprocessor = charger_modeles(marque)
        prediction = modele.predict(preprocessor.transform(data.to_pandas()))
        prediction = np.round(prediction, decimals=2)
        return prediction
    except Exception as e:
        print(f"Erreur lors de la prÃ©diction du prix. {e}")
```
:::

Notre objectif Ã  partir de ce moment est de rendre notre travail accessible et agrÃ©able Ã  utiliser. ğŸ˜

## Streamlit ğŸš€

Streamlit est une bibliothÃ¨que Python qui simplifie considÃ©rablement le processus de crÃ©ation d'applications web interactives pour l'analyse de donnÃ©es et la visualisation. 


:::{.notes}
expliquer streamlit
- onglet acheteur : fonctionnement avec des requÃªtes, 
- onglet vendeur : import du modÃ¨le et prÃ©dictions estimation du prix 



:::
