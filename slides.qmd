---
title: "üöô CarScraping üöô"
subtitle: "<script>const currentDT = new Date().toLocaleDateString();document.write(currentDT);</script>"
author: "`Aybuk√© BICAT` & `Hassan TILKI`"
format:
  revealjs:
    slide-number: true
    mermaid-format: svg
    chalkboard: true
    transition: slide
    background-transition: fade
    theme: simple
    incremental: true  
    footer: "--- Machine Learning ---"
    logo: https://corentinducloux.fr/dossier_img/mecen_transparent.png
    controls : true
    preview-links: auto
    view-distance: 20
    fontsize: 2.0em
    reference-location: document
    include-in-header:
        - text: |
            <style>
            #title-slide .title {
            font-size: 1.75em;
            }
            </style>
           
jupyter: python3
---



## Introduction

:::{.notes}
on s'est int√©ress√© au march√© de la voiture et on s'est pos√© la question de ce qu'on peut pr√©dire sur les voitures. Quand on pense √† √ßa, on pense forc√©ment au prix, comment pr√©dire le prix d'un v√©hivule?

La plupart des fran√ßais utilisent la c√¥te argus mais est-ce que vous vous √™tes d√©j√† demand√© comment celle-ci fonctionne?

- Argus :
Elle est bas√©e sur le cours-moyen d'argus qui est constitu√© √† l'aide de professionnels du secteur, d'un concessionnaire allant jusqu'au constructeur, mais aussi √† l'aide de simples particuliers et des annonces de ventes de v√©hicule.


En apprenant cela, nous avons eu comme objectif de cr√©er notre propre estimateur de prix de voiture.

Pour ce faire, vu qu'on est que 2 jeunes √©tudiants et ne sommes pas entour√©s de professionnels, la seule option que nus avons est de scraper des annonces de voitures d'occasion en ligne.

Avant de vous montrer comment on a scrapper tout √ßa on va directement passer au + croustillant qui est une d√©monstration de l'application que nous avons cr√©√©e.

Pr√©sentation Streamlit:

on se retrouvr sur l'accueil de notre app, qui est une app streamlit. On va vous expliquer plus tard ce que c'est.
Sur notre app, nous avons 3 onglets : accueil, acheteur et vendeur.

D√©mo acheteur, d√©mo vendeur.
On va pouvoir visualiser

d√©mo acheteur : Hassan est-ce que tu es interess√© par l'achat d'un v√©hicule?


si lien inexistant : c'√©tait tellement une bonne affaire qu'elle est d√©j√† partie...

d√©mo vendeur : toutes les donn√©es pr√©sentes dans l'onglet acheteur ont servi √† entrainer les mod√®les de pr√©diction.

on a une sorte de transparence sur comment est calcul√©e la c√¥te.

Est-ce que vous avez une voiture √† vendre? ou des caract√©ristiques
:::



*CarScraping qu'est-ce que c'est ?*

- Comment se calcule la **C√¥te ARGUS** ?

:::{.fragment}
> Elle est bas√©e sur le cours-moyen d'argus qui est constitu√© √† l'aide de professionnels du secteur, d'un concessionnaire allant jusqu'au constructeur, mais aussi √† l'aide de simples particuliers et des annonces de ventes de v√©hicule.
:::

- **Inconv√©nients** : Payant, manque de transparence.

- **Objectif** : cr√©er notre propre c√¥te ARGUS gratuite et sans filtre  !


## Web Scraping üï∏Ô∏è {background-image="imgs/webscraping.jpeg" background-opacity="0.1"}

:::{.notes}
Alors maintenant qu'on a vu le produit final, on va le diss√©quer pour voir comment on en est arriv√© l√† avec une approche technique!

Vous avez vu le voir peut √™tre mais il y a des liens dans l'onglet acheteur qui renvoie vers le site "La Centrale".

Alors nous avons scrap√© ce site et l'int√©gralit√© des 300 000 annonces qui √©taient pr√©sentes sur le site au moment du scraping.

Pour la partie scraping


On va vous montrer le processus de scraping en 3 parties : r√©cup√©ration des noms des marques, puis la r√©cup√©ration des annonces, et enfin la r√©cup√©ration des caract√©ristiques.

pas d'utilisation de Selenium car le site bloque tous les bot.

:::

> C'est une technique d'extraction de donn√©es pr√©sentes sur des sites  web par l'utilisation d'un script ou programme informatique.

:::{.fragment}
Donn√©es r√©cup√©r√©es depuis **La Centrale** :
:::

- \+ de <span style="color:#4682B4">**300 000**</span> annonces
- \+ de <span style="color:#4682B4">**150**</span> marques
- \+ de <span style="color:#4682B4">**900**</span> mod√®les

:::{.fragment}
Extraction en **3 parties** :

  - R√©cup√©ration des noms des marques
  - R√©cup√©ration des annonces
  - R√©cup√©ration des caract√©ristiques
:::


## Processus du Scraping : 1/3

:::{.notes}

On a un module `webscraping.py` qui nous permet d'extraire toutes ces donn√©es depuis le site.


Il faut savoir que le nombre maximum de pages sur La Centrale est de 500.

Avec 16 annonces par page, on n'atteint pas les 300k annonces annonc√©s dans l'accueil du site.

Pour y parvenir, nous avons d√©cid√© de scraper 500 pages d'annonces par marque et par mod√®les de v√©hicule √† l'aide de requ√™tes http.

Pour obtenir ces diff√©rentes marques, il suffit juste d'aller dans l'accueil du site, de cliquer sur la listbox "Mod√®le" et d'ensuite t√©l√©charger la page.



:::

- Utilisation des 2 packages python : `request` et `bs4`.

- R√©cup√©ration de la liste des marques et des mod√®les associ√©s.

:::{.fragment}
![](imgs/modelelistbox.png){fig-align="center" height="240"}
:::

- Transformer en format JSON.



:::{.fragment}
```{json}
{   "AC": [
        "aceca",
        "cobra"
    ],
    "AIWAYS": [
        "u5"
    ], ...}
```

:::

## Processus du Scraping : (2/3)
:::{.notes}

Une fois qu'on a r√©cup√©r√© les marques, on peut maintenant √† l'aide d'une boucle et de fonctions bien ficell√©es, extraire toutes les annonces sur le site. 
*extrait f¬∞ recup_pages()*

On va r√©cup√©rer les annonces directement depuis l'url pour pouvoir pallier le probl√®me des 500 pages maximum.

Cette fonction : `extraire_toutes_annonces()`, en contient d'autre. Ce qui permet d'iterer sur chaque marques et modeles disponible, grace au fichier JSON cr√©e pr√©alablement. Ensuite on effectue une requete http a l'aide de requetes (on montre l'exemple de la fonction recup_page).

:::

- Extraction de toutes les annonces disponibles sur le site √† l'aide de la fonction `extract_toutes_annonces()`, elle permet :
  - d'iterer sur chaque marques et mod√®les de v√©hicule
  - d'extraire les pages correspondantes
  
:::{.fragment}
```{py}
def recup_page(numero_page: int, marque: str, modele: str) -> BeautifulSoup:
    adresse = f"https://www.lacentrale.fr/listing?makesModelsCommercialNames={marque.upper()}%3A{modele.upper()}&options=&page={numero_page}"
    ...
    requete = rq.get(url=adresse)
    page = BeautifulSoup(requete.content, "html.parser")
``` 
  - d'avoir une sortie sous forme de liste d'annonces
:::

:::{.fragment}
```{py}
def recup_annonces(page: BeautifulSoup) -> list:
    annonces = page.find_all('div', class_='searchCardContainer')
    return list(annonces)
``` 

:::

## Processus du Scraping : (3/3)

:::{.notes}

Une fois toutes les pages r√©cuper√©es, sous cette forme de liste **pointe la slide**, on r√©cup√®re chacune des caract√©ristiques du v"hicule pour chq annonce. 
La fonction permettant de r√©cuperer toutes les caract√©ristiques d'une annonce s'appelle recup_info_voitures() qui elle aussi fait appel √† pls fonctions d√©di√©es √† pls caract√©ristiques. Ces fonctions permette √† l'aide de bs4, de recuperer des information pr√©cise stock√© dans certain √©lement de la page. 

Chacune des caract√©ristiques des voitures est stock√©e dans une dataclass `voiture`. 
:::

![](imgs/sorties_annonces.png){fig-align="center" width="0.5"}

- Extraction des caract√©ristiques des v√©hicules.

:::{.fragment}

```{py}
def recup_informations_voiture(annonce: str) -> voiture:
    marque = recup_nom_vehicule(annonce)
    cylindre = recup_cylindre(annonce)
    annee, kilometrage, boite, energie = recup_caracteristiques(annonce)
    prix = recup_prix(annonce)
    position_march√© = recup_position_march√©(annonce)
    garantie = recup_garantie(annonce)
    lien = recup_href(annonce)
    return voiture(marque=marque, cylindre=cylindre, annee=annee, kilometrage=kilometrage, boite=boite, energie=energie, prix=prix, position_march√©=position_march√©, garantie=garantie, lien=lien)

def recup_nom_vehicule(annonce: str) -> str: 
    nom_vehicule = annonce.find_all('h2', class_='Text_Text_text Vehiculecard_Vehiculecard_title Text_Text_subtitle2')
    return nom_vehicule[0].text
```

:::
----


:::{.notes}
Et une fois le tout execut√©, les donn√©es sont export√©s sous format brut JSON. 

le nb d'annonces √©tant tr√®s cons√©quent et l'extraction √©tant tr√®s longue, nous avons fait l'extraction en 4 parties. Nous avons obtenus donc  'fichiers json qu'on a fusionner en un fichier √† l'aide de la f¬∞ `fusionner_fichiers_json()`. 
:::

‚úÖ R√©sultats 4 fichiers JSON obtenu :


```
json/
  {} data_a_d.json
  {} data_e_l.json
  {} data_m_p.json
  {} data_r_z.json
```


Un extrait du fichier JSON obtenu : 
```{json}
[
    {
        "marque": "CITROEN C3 III",
        "cylindre": "1.2 PURETECH 110 FEEL",
        "annee": "2019",
        "kilometrage": "10 698 km",
        "boite": "Automatique",
        "energie": "Essence",
        "prix": "15 990 ‚Ç¨",
        "position_march√©": "Bonne affaire",
        "garantie": "Garantie 12 mois",
        "lien": "https://www.lacentrale.fr/auto-occasion-annonce-69112858137.html"
    },
    ...
]

```

On les fusionne √† l'aide de la fonction `fusionner_fichiers_json` pour obtenir un seul fichier contenant toutes les donn√©es extraites.



## üßº Data cleaning üßπ{background-image="imgs/datacleaning3.jpeg" background-opacity="0.2"}


:::{.notes}

Une fois notre fichier json obtenue, chose inevitable nous avons des donn√©es mal renseign√©es, des pb de types de donn√©es mais aussi des informations √† extraire de ces donn√©es brutes.

Pour r√©cup√©rer le + d'informations possibles sur ces donn√©es, nous avons d√©cid√© d'utiliser la librairie polars, ce qui rend le tout tr√®s lisible et tr√®s efficace.

:::

- Essentiel pour pouvoir exploiter les donn√©es.
- Permet de r√©cup√©rer + d'informations.

:::{.fragment}
Pour le nettoyage de la base de donn√©es, nous avons utilis√© le package `Polars` üêª‚Äç‚ùÑÔ∏è, qui poss√®de de nombreux avantages comme :  
:::
-   la cr√©ation de pipelines üõ¢Ô∏è
-   un code plus lisible üëÄ
-   dans certains cas, + efficace que `Pandas` üêº

:::{.fragment}
C'est le module `datacleaning.py` qui contient toutes les fonctions essentiels pour r√©soudre certains probl√®mes.
:::

## Les probl√®mes rencontr√©es ‚ö†Ô∏è

:::{.notes}

Dans cette partie, nous allons vous pr√©senter rapidement les pb qu'on a rncontr√©es, et les fonctions qui ont servi √† les r√®gler et qui nous ont permis de nettoyer la base de donn√©es et d'avoir une BDD exploitable.


- modele_marque_generation

pb rencontr√©, nom de la f¬∞, et la sortie.  (faire figurer l'ex des docstring)

:::

- S√©parer la marque, le mod√®le et la g√©n√©ration √† l'aide de `get_marque_modele_generation()`

:::{.fragment}
$\rightarrow$ [`'CITROEN C3 III'`]{.fragment .semi-fade-out} [->]{.fragment} [`('CITROEN', 'C3', 'III')`]{.fragment}
:::

- Corriger le type des variables `kilometre`, `prix` et `annee` avec `get_km_prix_annee` : 

:::{.fragment}
$\rightarrow$ [`"100 000 km", "15 000 ‚Ç¨", "2015"`]{.fragment .semi-fade-out} [->]{.fragment} [`(100000, 15000, 2015)`]{.fragment}
:::

- Diviser la chaine de caractere de `cylindre` en plusieurs variables avec `get_cylindre`: 

:::{.fragment}
$\rightarrow$ [`"6.3 V8 460 GT"`]{.fragment .semi-fade-out} [->]{.fragment} [`('6.3', 'V8', 460, 'GT')`]{.fragment}
:::


## Le gazoduc üõ¢Ô∏è{background-image="imgs/gazoduc1.jpeg" background-opacity="0.1"}

:::{.notes}
f¬∞ qui permet de faire tout le traitement automatiquement √† l'aide des autres fonctions. 

Comme on peut le voir, il y a d'autre fonction dans cette pipeline que l'on ne vous a pas pr√©sent√©. On va donc le faire tr√®s rapidement. 

get_garantie traite les donn√©es de la colonne "garantie" dans en transformant le Garantie 12 mois en gardant uniquement les valeurs num√©riques. Elle contient aussi d'autre filtre pour √©carter les valeurs ab√©rantes 

filter_data permet de supprimer les valeurs ab√©rante pour le kilom√©trage et l'ann√©e 2024. 

supp_doublons permet de supprimer les annonces en double en ce basant sur les liens des annonces. 

supp_na: Elle permet aussi de supprimer les valeurs manquantes pour la puissance. et pour les marques et modeles de v√©hicule

Une fois le dataframe nettoy√© il sera export√© en format parquet car parquet a une Compression efficace et permet d'avoir des √©conomies de stockage. 
:::


C'est une fonction qui applique un pipeline de traitement de donn√©es sur le DataFrame donn√©.

:::{.fragment}
```py
def gazoduc(data: pl.DataFrame, nom_marques_modeles: pl.DataFrame) -> pl.DataFrame:
    data = (data.pipe(get_marque_modele_generation, nom_marques_modeles)üõ¢Ô∏è
                .pipe(get_km_prix_annee)                                üõ¢Ô∏è
                .pipe(get_garantie)                                     üõ¢Ô∏è
                .pipe(get_cylindre)                                     üõ¢Ô∏è
                .pipe(filter_data)                                      üõ¢Ô∏è
                .pipe(supp_doublons)                                    üõ¢Ô∏è  
                .pipe(supp_na)                                          üõ¢Ô∏è
        )
    return data
```
:::

:::{.fragment}
<div style="overflow-x: auto;">
  <table style="max-width: 800px; font-size: 18px;">
    <caption>Extrait du dataframe</caption>
    <thead>
      <tr>
        <th>ann√©e</th>
        <th>kilom√©trage</th>
        <th>bo√Æte</th>
        <th>√©nergie</th>
        <th>prix</th>
        <th>position march√©</th>
        <th>garantie</th>
        <th>lien</th>
        <th>marque</th>
        <th>mod√®le</th>
        <th>g√©n√©ration</th>
        <th>cylindre</th>
        <th>moteur</th>
        <th>puissance</th>
        <th>finition</th>
        <th>batterie</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>2020</td>
        <td>107410</td>
        <td>Manuelle</td>
        <td>Essence</td>
        <td>13050</td>
        <td>Offre √©quitable</td>
        <td>12</td>
        <td><a href="https://www.lacentrale.fr/auto-occasion-annonc...">Lien vers l'annonce</a></td>
        <td>CITROEN</td>
        <td>C3</td>
        <td>III</td>
        <td>1.2</td>
        <td>PURETECH</td>
        <td>82</td>
        <td>S&S FEEL BUSINESS</td>
        <td>None</td>
        <td>-</td>
      </tr>
    </tbody>
  </table>
</div>
:::

:::{.fragment}
Ce dataframe sera export√© en format `.parquet` qui permet de stocker des bases volumineuse √† moindre co√ªt. 
:::

## üìñ Machine learning üìñ{background-image="imgs/ML1.jpeg" background-opacity="0.1"}



:::{.notes}
- split
- les mod√®les -> pourquoi avoir choisi ces mod√®les ? car apr√®s test, ces mod√®les se sont aver√©e √™tre meilleurs.
- les grilles de param√®tres associ√©es -> pourquoi ces grilles ? car apr√®s test, se sont les r√©sultats qui sont sortie le plus souvent.
- trouver le meilleur mod√®le (CV) -> on montre la fonction 
- export du Mod√®le et pr√©processeur
- import du mod√®le et utilisation pour pr√©diction

:::

Pour des raisons d'efficacit√© pour la pr√©diction, **chaque marque de v√©hicule aura son propre mod√®le de pr√©diction**.

Ces mod√®les de pr√©diction ne peuvent √™tre choisi au hasard. Il faut effectuer des tests et choisir le mod√®le le plus adapt√©.



- package utilis√© : `sklearn`

:::{.fragment}
L'utilisation de ce package √† plusieurs avantages, tel que sa documentation et une utilisation avec des **pipelines** possibles. Cependant, il ne fonctionne pas avec des bases de donn√©es `Polars`.
:::

- split des donn√©es avec `split_data`
- preprocesseur avec `get_preprocessor`
- mod√®les et grille de param√®tres avec `set_models` et `get_params`
- pour r√©cup√©rer et exporter les meilleurs mod√®les : `get_all_models`
- `predict_prix` pour charger le mod√®le et pr√©dire

## Le split üë®‚Äçüë©‚Äçüëß‚Äçüë¶

:::{.fragment}
**Donn√©es d'entrainement** : √©chantillon de <span style="color:#4682B4">**80%**</span> d'observations de la marque

**Donn√©es de test** : les <span style="color:#4682B4">**20%**</span> restants.
:::

:::{.fragment}
Le split est effectu√© √† l'aide de `split_data` : 

```py
def split_data(data: pl.DataFrame, marque: str) -> tuple[pl.DataFrame, pl.DataFrame, pd.DataFrame, pd.DataFrame, np.ndarray, np.ndarray]:

    X = data.filter(pl.col("marque") == marque)
    y = X.select('prix')
    X = X.select(pl.exclude("position_march√©")
                ).select(pl.exclude("lien")
                        ).select(pl.exclude("garantie")
                                ).select(pl.exclude("prix"))

    # Split des donn√©es en ensembles d'entra√Ænement et de test
    X_train, X_test, y_train, y_test = train_test_split(X.to_pandas(), y.to_numpy(), test_size=0.2, random_state=21)
    return X, y, X_train, X_test, y_train, y_test

```
:::

## Le preprocesseur ‚èπÔ∏è

La plupart des mod√®les ne supportent pas les variables qualitatives et requiert donc un pr√©traitement. 

:::{.fragment}
`OneHotEncoder` est utilis√© pour convertir les caract√©ristiques cat√©gorielles en une repr√©sentation binaire.

Exemple : 

$\rightarrow$ [`'boite' = 'Automatique''`]{.fragment .semi-fade-out} [->]{.fragment} [`boite.Automatique = 1`]{.fragment}
:::

:::{.fragment}
`StandardScaler` est utilis√© pour effectuer une mise √† l'√©chelle des caract√©ristiques num√©riques. Il standardise les donn√©es en soustrayant la moyenne et en divisant par l'√©cart type, de sorte que chaque caract√©ristique ait une moyenne de z√©ro et une variance de un.
:::

## Les mod√®les üìä

:::{.notes}
Nous avons test√© diff√©rents mod√®les tel que les SVM, les Boosting, etc.. Mais les meilleurs mod√®les √† chaque fois √©taient : (ceux sur la slide)

Le nombre de mod√®le √† creer √©tant de 40, le proc√©d√©s est assez long. C'esst pour cela que pour optimiser notre temps nous avons d√©cider de tester certaines grille de parametres en amont pour en determiner une finale, la plus r√©duite possible.

Nous allons donc rapidement vous pr√©sentez les r√©sultats pr√©liminaires qui nous on permis d'avoir une grille finale.

:::

Apr√®s quelques tests, nous avons d√©cid√© d'utiliser uniquement les mod√®les suivant : 

- Regression lin√©aire üìà
- K-neighbors üë¨üë≠
- Random Forest üå≥

:::{.fragment}
La quantit√© de donn√©es √©tant importantes, l'optimisation des param√®tres peut-√™tre tr√®s longues.‚è≥
::: 

:::{.fragment}
C'est pour cette raison que plusieurs grilles ont √©t√© test√© en amont afin de determiner une grille finale, voici les r√©sultats interm√©diaires.
:::

## Le nombre de voisins üë¨üë¨üë¨

:::{.notes}
Au lieu de se lancer directement dans des validations crois√©es avec un grille de param√®tres choisi al√©atoirement, 


Nous avons test√© des grilles de param√®tres entre 0 et 15 voisins. Le maximum retenu √©tait 10 voisins. Cette pour cette raison que nous avons retenu cette grille.
:::

:::{.fragment}
```{python}
import numpy as np
import polars as pl
import duckdb
import plotly.express as px
marques_array = duckdb.sql(
    """Select marque from (
    SELECT COUNT(*) as nb_annonces,
    marque
    FROM 'data/database.parquet'
    GROUP BY marque
    ORDER BY nb_annonces DESC)
    """).pl().head(40)
marques_array = marques_array.to_numpy()
def cv_result_into_df(modele: str, marques_array: np.ndarray): 
    for marque in marques_array:
        if marque[0] == marques_array[0,0]:
            cv_results = duckdb.sql(
                f"""
                SELECT '{marque[0]}' as marque,
                mean_test_score,
                '{modele}' as modele,
                params
                FROM 'cv_results_neighbors/{marque[0]}_{modele}_results.json'
                WHERE rank_test_score = 1
                """).pl().unnest('params')
        else :
            cv_results_2 = duckdb.sql(
                f"""
                SELECT '{marque[0]}' as marque,
                mean_test_score,
                '{modele}' as modele,
                params
                FROM 'cv_results_neighbors/{marque[0]}_{modele}_results.json'
               
                WHERE rank_test_score = 1
                """).pl().unnest('params')
            cv_results = pl.concat([cv_results, cv_results_2], rechunk=True)
    return cv_results

px.bar(cv_result_into_df("KNeighbors", marques_array), x='marque', y='mean_test_score', color = 'n_neighbors', color_continuous_scale=px.colors.sequential.Blues)
```
:::

:::{.fragment}
La grille retenue: `'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]`
:::

## min_samples_leaf üçÉ

:::{.fragment}
```{python}

def cv_result_into_df(modele: str, marques_array: np.ndarray): 
    for marque in marques_array:
        if marque[0] == marques_array[0,0]:
            cv_results = duckdb.sql(
                f"""
                SELECT '{marque[0]}' as marque,
                mean_test_score,
                '{modele}' as modele,
                params
                FROM 'cv_results_leaf/{marque[0]}_{modele}_results.json'
                WHERE rank_test_score = 1
                """).pl().unnest('params')
        else :
            cv_results_2 = duckdb.sql(
                f"""
                SELECT '{marque[0]}' as marque,
                mean_test_score,
                '{modele}' as modele,
                params
                FROM 'cv_results_leaf/{marque[0]}_{modele}_results.json'
               
                WHERE rank_test_score = 1
                """).pl().unnest('params')
            cv_results = pl.concat([cv_results, cv_results_2], rechunk=True)
    return cv_results

px.bar(cv_result_into_df("RandomForest", marques_array), x='marque', y='mean_test_score', color = 'min_samples_leaf', color_continuous_scale=px.colors.sequential.Mint)
```
:::

:::{.fragment}
La grille retenue: `'min_samples_leaf': [2]`
:::

## n_estimators
:::{.notes}
Ici on avait test√© plusieurs valeurs allant de 100 √† 1000. 

On a d√©cid√© de garder uniquement 500 et 800

:::

:::{.fragment}
```{python}

def cv_result_into_df(modele: str, marques_array: np.ndarray): 
    for marque in marques_array:
        if marque[0] == marques_array[0,0]:
            cv_results = duckdb.sql(
                f"""
                SELECT '{marque[0]}' as marque,
                mean_test_score,
                '{modele}' as modele,
                params
                FROM 'cv_results_nestimators_2/{marque[0]}_{modele}_results.json'
                WHERE rank_test_score = 1
                """).pl().unnest('params')
        else :
            cv_results_2 = duckdb.sql(
                f"""
                SELECT '{marque[0]}' as marque,
                mean_test_score,
                '{modele}' as modele,
                params
                FROM 'cv_results_nestimators_2/{marque[0]}_{modele}_results.json'
               
                WHERE rank_test_score = 1
                """).pl().unnest('params')
            cv_results = pl.concat([cv_results, cv_results_2], rechunk=True)
    return cv_results

px.bar(cv_result_into_df("RandomForest", marques_array), x='marque', y='mean_test_score', color = 'n_estimators', color_continuous_scale=px.colors.sequential.Blues)
```
:::

:::{.fragment}
La grille retenue: `'n_estimators': [500, 800]`
:::

## L'utilisation des mod√®les

:::{.notes}
Une fois l'optimisation des param√®tres effectu√© via une seconde validation crois√©e. On garde le meilleur mod√®le parmis les trois, on l'entraine avec tout le dataset pour la marque en question, et on l'export en fichier joblib, ainsi que le preprocesseur pour r√©utilisation. 
:::

Une fois que les meilleurs mod√®les ont √©t√© trouv√©s on les exporte :

:::{.fragment}
```py
def export_models(model: LinearRegression|KNeighborsRegressor|RandomForestRegressor, preprocessor: ColumnTransformer, marque: str) -> None:
  # Sauvegarder le mod√®le
  dump(model, f'models/{marque}_best_model.joblib')
  # Sauvegarder le pr√©processeur
  dump(preprocessor, f'models/{marque}_preprocessor.joblib')
```
:::

:::{.fragment}
Pour ensuite les importer lorsque l'on voudra pr√©dire les prix ! üí∞
:::

:::{.fragment}
```py
def predict_prix(data: pl.DataFrame, marque: str) -> np.ndarray:
    try:
        modele, preprocessor = charger_modeles(marque)
        prediction = modele.predict(preprocessor.transform(data.to_pandas()))
        prediction = np.round(prediction, decimals=2)
        return prediction
    except Exception as e:
        print(f"Erreur lors de la pr√©diction du prix. {e}")
```
:::

Notre objectif √† partir de ce moment est de rendre notre travail accessible et agr√©able √† utiliser. üòé

## Streamlit üöÄ

Streamlit est une biblioth√®que Python qui simplifie consid√©rablement le processus de cr√©ation d'applications web interactives pour l'analyse de donn√©es et la visualisation. 


:::{.notes}
expliquer streamlit
- onglet acheteur : fonctionnement avec des requ√™tes, 
- onglet vendeur : import du mod√®le et pr√©dictions estimation du prix 



:::
